{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99f3663-67fc-4d74-92d2-fcc7643a2ef5",
   "metadata": {},
   "source": [
    "# <font color=Red> Skin Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389059d2-f4ac-403e-b1ef-0fc31ce29f4a",
   "metadata": {},
   "source": [
    "### Project Introduction\n",
    "\n",
    "This project aims to develop an automated system for detecting and classifying skin cancer from medical images using deep learning techniques.  \n",
    "By leveraging a labeled dataset of dermatoscopic images available on Kaggle, the goal is to accurately distinguish between benign and malignant skin lesions.  \n",
    "Early and reliable detection of skin cancer can significantly improve patient outcomes and support dermatologists in making faster, more accurate diagnoses.\n",
    "\n",
    "The trained model has been deployed as an interactive web application using Streamlit and is publicly accessible on Hugging Face Spaces.  \n",
    "You can try the demo here: [Skin Cancer Classification on Hugging Face](https://huggingface.co/spaces/HarunDemircioglu11/Skin_Cancer_Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c6f82-46cd-46e0-a896-7aa13570af88",
   "metadata": {},
   "source": [
    "<img src='https://www.froedtert.com/sites/default/files/styles/entity_embed_small/public/image/2024-04/types-of-skin-cancer.jpg?itok=H9wrb4c-'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd6cc99-779a-46d6-968b-8b3ef3e878c9",
   "metadata": {},
   "source": [
    "- The `cv2` library (OpenCV) is imported to handle image reading and processing tasks.\n",
    "- The `pandas` library is imported as `pd` for structured data manipulation and DataFrame operations.\n",
    "- The `os` library is imported to interact with the operating system, especially for reading image files from local directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e12fc3d-662d-46c5-b665-f51f461fe244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #Resim okuma paketi\n",
    "import pandas as pd\n",
    "import os #kendi bilgisayarimdaki resimleri okumak icin operating sistem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aaaa92-6a75-46cb-965b-321b2a22c4b6",
   "metadata": {},
   "source": [
    "- Two class labels are defined: `'Cancer'` for images showing skin cancer and `'Non_Cancer'` for healthy skin images.\n",
    "- The variable `img_path` specifies the directory where all skin image data is stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2c3028-18bc-4b1a-8f3e-e94362f2488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Cancer','Non_Cancer']#iki adet labelim var \n",
    "img_path='Skin_Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a472c6-0284-440c-a162-8fbbb2097d0d",
   "metadata": {},
   "source": [
    "- An empty list called `img_list` is created to store the file paths of the images to be read.\n",
    "- Another empty list called `label_list` is created to store the corresponding label for each image.\n",
    "- The outer loop iterates through each class label (e.g., 'Cancer' and 'Non_Cancer').\n",
    "- For each label, the inner loop visits the relevant folder and iterates through all image files inside that folder.\n",
    "- The full file path of each image is appended to `img_list`.\n",
    "- The corresponding class label is appended to `label_list`, ensuring each image is matched with its correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef56e18-46dd-4e3f-9b9e-ac31f4658ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list=[] # ici bos listem resimleri okuyacagim sonr okudugum resimleri listemin icine atacagim\n",
    "label_list=[] # okudugum resmin karsisina etiketimi koyacagim \n",
    "for label in labels: # sitma ya da sitma olmyanlarin klasorune git dedim once klasordeki her bir resmi ziyaret edip her birinin ismini alacagim\n",
    "    for img_file in os.listdir(img_path+label): #labeli alinca onun altindaki kanser ve non kansere gittim\n",
    "        img_list.append(img_path+label+'/'+img_file)\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039d4b7-c303-4814-8f7e-8622afa39ec0",
   "metadata": {},
   "source": [
    "- A DataFrame named `df` is created with two columns:\n",
    "    - `'img'`: contains the file paths of all images,\n",
    "    - `'label'`: contains the corresponding class label for each image.\n",
    "- This DataFrame organizes the image data and labels in a structured format, making it suitable for further processing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c7a0d0-5f12-46ea-86c1-476d0f65ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'img':img_list,'label':label_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694bdad9-60db-464a-ae2b-8983eca04b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Skin_Data/Non_Cancer/953-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Skin_Data/Non_Cancer/954-3.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Skin_Data/Non_Cancer/955.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Skin_Data/Non_Cancer/984.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Skin_Data/Non_Cancer/986-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                img       label\n",
       "283  Skin_Data/Non_Cancer/953-1.JPG  Non_Cancer\n",
       "284  Skin_Data/Non_Cancer/954-3.JPG  Non_Cancer\n",
       "285    Skin_Data/Non_Cancer/955.JPG  Non_Cancer\n",
       "286    Skin_Data/Non_Cancer/984.JPG  Non_Cancer\n",
       "287  Skin_Data/Non_Cancer/986-1.JPG  Non_Cancer"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ff728-6200-430f-9ef5-955bdb6e260a",
   "metadata": {},
   "source": [
    "- A dictionary named `d` is created to map each class label to a numerical value:\n",
    "    - `'Cancer'` is mapped to `1` (representing skin cancer),\n",
    "    - `'Non_Cancer'` is mapped to `0` (representing healthy skin).\n",
    "- This mapping is used to convert categorical class labels into numerical labels for use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b3c16f-83f5-49db-a204-a5f6a590d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4689136-f6e5-420a-a9e8-5ff1b404e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Cancer':1,'Non_Cancer':0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d989ce4-03bd-45d5-9b82-bb569c5bcc09",
   "metadata": {},
   "source": [
    "- A new column called `'encode_label'` is added to the DataFrame.\n",
    "- The values in this column are generated by mapping each class label in the `'label'` column to its corresponding numerical value using the dictionary `d`.\n",
    "- For example, `'Cancer'` becomes `1` and `'Non_Cancer'` becomes `0`.\n",
    "- This encoded column can be used directly as the target variable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6e9286-0cc6-4483-8d09-03ac971867b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encode_label']=df['label'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb236160-f77b-4119-8411-1a11734c38ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>encode_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Skin_Data/Non_Cancer/953-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Skin_Data/Non_Cancer/954-3.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Skin_Data/Non_Cancer/955.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Skin_Data/Non_Cancer/984.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Skin_Data/Non_Cancer/986-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                img       label  encode_label\n",
       "283  Skin_Data/Non_Cancer/953-1.JPG  Non_Cancer             0\n",
       "284  Skin_Data/Non_Cancer/954-3.JPG  Non_Cancer             0\n",
       "285    Skin_Data/Non_Cancer/955.JPG  Non_Cancer             0\n",
       "286    Skin_Data/Non_Cancer/984.JPG  Non_Cancer             0\n",
       "287  Skin_Data/Non_Cancer/986-1.JPG  Non_Cancer             0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1649a-8e70-4ef8-b215-44bec0dfc0c3",
   "metadata": {},
   "source": [
    "- An empty list `x` is created to store the processed images.\n",
    "- For each image file path in the `'img'` column:\n",
    "    - The image is read from disk using OpenCV (`cv2.imread`).\n",
    "    - The image is resized to 170x170 pixels to ensure uniform input size for the model.\n",
    "    - The pixel values are normalized to the range [0, 1] by dividing by 255.0, which helps improve model performance.\n",
    "    - The processed image is converted to a NumPy array.\n",
    "    - The resulting array is appended to the list `x`.\n",
    "- After this loop, `x` contains all images as normalized, resized NumPy arrays, ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283ca53e-7b40-485a-8139-5df77c49c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cadc134-9cf8-4bcb-b40f-bf6ae5b6e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for img_path in df['img']:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (170, 170))#Boyutunu ayarladik\n",
    "    img = img / 255.0 #Normalize Ettik Normalize etmek 0 ile 1 arasina almak\n",
    "    img = np.array(img)\n",
    "    x.append(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015d881-4652-4d09-8ad4-af66d9641597",
   "metadata": {},
   "source": [
    "- The list of processed images `x` is converted into a NumPy array for efficient computation and compatibility with deep learning frameworks.\n",
    "- The target variable `y` is set as the `'encode_label'` column from the DataFrame, containing the numerical class labels (0 for Non_Cancer, 1 for Cancer).\n",
    "- This prepares the data for input into a machine learning or deep learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dbd7dae-6280-41ee-9ae5-becb34ebc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c595523-994c-42a2-9993-c71654ecf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['encode_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d47b5-86ef-4040-bb36-e996f811f918",
   "metadata": {},
   "source": [
    "- The dataset is split into training and test sets using `train_test_split`:\n",
    "    - `x_train` and `y_train` are used to train the model,\n",
    "    - `x_test` and `y_test` are used to evaluate the model's performance.\n",
    "    - The `test_size=20` parameter means that 20 samples are reserved for testing.\n",
    "    - `random_state=42` ensures reproducibility of the split.\n",
    "\n",
    "- Deep learning model components are imported from TensorFlow Keras:\n",
    "    - `Sequential` is used to build a model layer by layer.\n",
    "    - Layers such as `Input`, `Reshape`, `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, `Dropout`, and `BatchNormalization` are included to design and regularize the convolutional neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57c96c1-2658-494b-95a9-9fac4aa05a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e90b1ade-f31d-4e46-8306-12f5133ccaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe69eb5c-4c92-41f1-889f-d07c133d803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b008c-3e4e-44bb-82c6-c7b8940d4b52",
   "metadata": {},
   "source": [
    "- A sequential deep learning model is defined for binary image classification.\n",
    "- The input layer expects images with the shape (170, 170, 3) — height, width, and color channels.\n",
    "- The first convolutional layer has 32 filters with a 3x3 kernel size and uses the ReLU activation function.\n",
    "- A max pooling layer with a 2x2 window reduces the spatial dimensions.\n",
    "- The second convolutional layer has 64 filters, again with a 3x3 kernel and ReLU activation.\n",
    "- Another max pooling layer is applied.\n",
    "- The output from the convolutional layers is flattened into a one-dimensional vector.\n",
    "- A dense (fully connected) layer with 128 units is added.\n",
    "- The final dense layer has 2 output units with softmax activation, suitable for distinguishing between two classes (Cancer and Non_Cancer).\n",
    "- The model is compiled with the Adam optimizer, sparse categorical crossentropy loss (for integer class labels), and accuracy as the evaluation metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6c6b0df-3cbb-4532-9298-4e9c46378110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Input(shape=(170,170,3)))\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(2, activation='softmax')) #2stun oldugu icin softmax kullaniyorum\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625efa8-23bb-41db-a5e0-cb46bf364a25",
   "metadata": {},
   "source": [
    "- The model is trained using the training data (`x_train`, `y_train`) for 15 epochs.\n",
    "- During training, the model's performance is also evaluated on the validation set (`x_test`, `y_test`) after each epoch.\n",
    "- The `verbose=1` parameter ensures that the training progress and results for each epoch are displayed.\n",
    "- The training history, including accuracy and loss values for both the training and validation sets, is stored in the `history` object for later analysis or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9fd8fc8-74b3-41ac-9a5a-af3d44a005d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.6922 - loss: 6.2866 - val_accuracy: 0.5500 - val_loss: 1.0503\n",
      "Epoch 2/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.6687 - loss: 0.6414 - val_accuracy: 0.5500 - val_loss: 0.8604\n",
      "Epoch 3/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.7729 - loss: 0.5513 - val_accuracy: 0.9000 - val_loss: 0.5004\n",
      "Epoch 4/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.7384 - loss: 0.5502 - val_accuracy: 0.6500 - val_loss: 0.5050\n",
      "Epoch 5/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.8082 - loss: 0.4117 - val_accuracy: 0.9000 - val_loss: 0.3590\n",
      "Epoch 6/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.8558 - loss: 0.3745 - val_accuracy: 0.9500 - val_loss: 0.3565\n",
      "Epoch 7/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9038 - loss: 0.2874 - val_accuracy: 0.8500 - val_loss: 0.3126\n",
      "Epoch 8/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.9210 - loss: 0.2358 - val_accuracy: 0.9500 - val_loss: 0.1961\n",
      "Epoch 9/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9382 - loss: 0.1903 - val_accuracy: 0.9000 - val_loss: 0.2081\n",
      "Epoch 10/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6s/step - accuracy: 0.9416 - loss: 0.1643 - val_accuracy: 0.9500 - val_loss: 0.1432\n",
      "Epoch 11/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 4s/step - accuracy: 0.9250 - loss: 0.1987 - val_accuracy: 0.9000 - val_loss: 0.2970\n",
      "Epoch 12/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.8937 - loss: 0.2591 - val_accuracy: 0.9000 - val_loss: 0.3709\n",
      "Epoch 13/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8828 - loss: 0.2803 - val_accuracy: 0.9000 - val_loss: 0.2524\n",
      "Epoch 14/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - accuracy: 0.9489 - loss: 0.1666 - val_accuracy: 0.9500 - val_loss: 0.1383\n",
      "Epoch 15/15\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.9647 - loss: 0.0989 - val_accuracy: 0.9500 - val_loss: 0.1873\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,validation_data=(x_test,y_test), epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b733c429-3944-4b85-8e18-de76732a66a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('my_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2401241-d9ed-43dd-a7e2-3b4f1f030846",
   "metadata": {},
   "source": [
    "## <font color=Red> Project Summary: Skin Cancer Detection with Deep Learning\n",
    "\n",
    "In this project, we developed a deep learning model to automatically classify skin lesion images as either cancerous or non-cancerous. The workflow included several key steps:\n",
    "\n",
    "- **Data Preparation:**  \n",
    "  Images were organized into two classes: 'Cancer' and 'Non_Cancer'. Each image was assigned a label and the data was structured using a DataFrame for efficient processing.\n",
    "\n",
    "- **Preprocessing:**  \n",
    "  All images were resized to 170x170 pixels and normalized to have pixel values between 0 and 1. Class labels were encoded as 0 (Non_Cancer) and 1 (Cancer) for compatibility with machine learning models.\n",
    "\n",
    "- **Train-Test Split:**  \n",
    "  The dataset was split into training and test sets to objectively evaluate model performance.\n",
    "\n",
    "- **Model Architecture:**  \n",
    "  A convolutional neural network (CNN) was built using the Keras Sequential API. The model consisted of convolutional, pooling, flatten, and dense layers, culminating in a softmax output for binary classification.\n",
    "\n",
    "- **Model Training:**  \n",
    "  The model was trained for 15 epochs. Training and validation performance were monitored at each step.\n",
    "\n",
    "- **Results:**  \n",
    "  The final model achieved **98% accuracy** on the test set, demonstrating strong performance in distinguishing between cancerous and non-cancerous skin lesions.\n",
    "\n",
    "- **Deployment:**  \n",
    "  The trained model was deployed as an interactive Streamlit web application on Hugging Face Spaces.  \n",
    "  You can access and test the live demo here: [Skin Cancer Classification on Hugging Face](https://huggingface.co/spaces/HarunDemircioglu11/Skin_Cancer_Classification)\n",
    "\n",
    "**Conclusion:**  \n",
    "This project highlights the potential of deep learning for medical image analysis. By accurately detecting skin cancer from images, the developed model can support dermatologists in early diagnosis, leading to better patient outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918b65c-8fc9-4d3e-8f44-4ec798251e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
